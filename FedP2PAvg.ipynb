{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### FedP2PAvg vs. FedAvg \n",
        "\n",
        "This notebook provides a clean, **fully-commented** baseline to run either the **traditional FedAvg** or the **peer-to-peer (P2P) refinement** variant **FedP2PAvg** in **federated learning under non-IID Dirichlet partitions**.\n",
        "\n",
        "The structure intentionally mirrors your function naming and flow so participants can quickly extend it during the hackathon:\n",
        "\n",
        "- Toggle: `FAST_MODE` (speed vs determinism).\n",
        "- Models: `NetMnist` (LeNet-like), `NetMnistFC`, `BasicBlock`, `NetCifar10` (VGG-11 like).\n",
        "- Load dataset: `load_dataset(dataset, n_particoes, dirichlet_alpha)` — Dirichlet partition (sequential consumption, per-partition shuffle).\n",
        "- Load networks and data Visualization:  `loadNetowrk`, `configure_nets`, `plot_class_distribution`.\n",
        "- Training modes: `train_nets` (classic local training), `train_p2p` (p2p training step).\n",
        "- Aggregation & evaluation: `combine`, `eval`,.\n",
        "- Visualization: `plot_class_distribution`.\n",
        "- **Main loop**\n",
        "\n",
        "> **Note:** All code is commented in **English** for clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksikCwbWAWEm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- FAST_MODE toggle ----------\n",
        "# When True: faster runs (cudnn benchmark on)\n",
        "# When False: deterministic behavior for reproducibility\n",
        "FAST_MODE = True\n",
        "\n",
        "# ---------- Seeds & Determinism ----------\n",
        "SEED = 2\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "if FAST_MODE:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "else:\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Windows\n",
        "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# MAC\n",
        "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Usando o dispositivo: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment Configuration (Quick Knobs for the Hackathon)\n",
        "\n",
        "This cell centralizes the **main hyperparameters** and **experiment switches** you’ll tweak during the hackathon. Adjust these to compare **FedAvg** vs **FedP2PAvg**, datasets, and heterogeneity levels.\n",
        "\n",
        "---\n",
        "\n",
        "#### Run Mode & Dataset\n",
        "- `RUN_MODE =`  \n",
        "  Options: `\"fedavg\"` or `\"fedp2pavg\"`.  \n",
        "  - **fedavg:** classic centralized non-weighted aggregation (baseline).  \n",
        "  - **fedp2pavg:** adds **peer-to-peer refinement** between clients before global aggregation; often improves stability/accuracy under non-IID data at modest comms cost.\n",
        "- `DATASET =`  \n",
        "  Options: `\"mnist\"`, `\"fashionmnist\"`, `\"cifar10\"`.  \n",
        "  - Use **MNIST/FashionMNIST** for faster iterations; **CIFAR-10** for a more challenging RGB benchmark.\n",
        "- `NUM_CLASSES = 10`  \n",
        "  Number of classes in the chosen dataset (MNIST/FashionMNIST/CIFAR-10 = 10).\n",
        "\n",
        "---\n",
        "\n",
        "#### Federated Setup\n",
        "- `N_CLIENTS = 10`  \n",
        "  Total simulated clients participating in training (per round, this can be all or a subset depending on your training loop).\n",
        "- `DIRICHLET_ALPHA = 0.1`  \n",
        "  Controls **non-IID p**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_MODE = \"fedavg\"      # options: \"fedavg\" or \"fedp2pavg\"\n",
        "DATASET  = \"cifar10\"       # options: \"mnist\", \"fashionmnist\", \"cifar10\"\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "N_CLIENTS = 10           # number of federated clients\n",
        "DIRICHLET_ALPHA = 0.1    # lower => more heterogeneous\n",
        "\n",
        "LOCAL_EPOCHS = 2         # local epochs per round\n",
        "BATCH_SIZE   = 128\n",
        "LR           = 0.01\n",
        "MOMENTUM     = 0.9\n",
        "\n",
        "GLOBAL_ROUNDS = 600       # keep modest for quick demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neural Network Architectures for FL Experiments\n",
        "\n",
        "This cell defines **four neural network building blocks** you can plug into your federated learning experiments, depending on the dataset and compute/communication budget on each client. All classes follow the standard PyTorch `nn.Module` pattern (`__init__` to declare layers, `forward` to define the data flow) and **return raw logits** (no softmax), which is what `nn.CrossEntropyLoss` expects.\n",
        "\n",
        "---\n",
        "\n",
        "#### `NetMnist` — lightweight CNN for MNIST (1×28×28)\n",
        "- **Purpose:** A small, regularized convolutional model that works well on MNIST while keeping parameters/updates compact for FL.\n",
        "- **Architecture flow:**\n",
        "  1. `Conv2d(1→10, k=5)` → `MaxPool2d(2)` → `ReLU`\n",
        "  2. `Conv2d(10→20, k=5)` → `Dropout2d` → `MaxPool2d(2)` → `ReLU`\n",
        "  3. **Flatten** to 320 features (`20×4×4` from the conv stack)\n",
        "  4. `Linear(320→50)` → `ReLU` → `Dropout`\n",
        "  5. `Linear(50→10)` → **logits**\n",
        "- **Shape trace (N=batch size):**\n",
        "  - Input: `(N, 1, 28, 28)`\n",
        "  - After `conv1`: `(N, 10, 24, 24)` → pool → `(N, 10, 12, 12)`\n",
        "  - After `conv2`: `(N, 20, 8, 8)` → pool → `(N, 20, 4, 4)` → flatten → `(N, 320)`\n",
        "  - FCs → `(N, 10)`\n",
        "- **Notes:** `Dropout2d` regularizes feature maps; use this when clients are very resource-constrained but you still want conv features.\n",
        "\n",
        "---\n",
        "\n",
        "#### `NetMnistFC` — fully connected MNIST baseline\n",
        "- **Purpose:** A tiny MLP baseline without convolutions; useful as a minimal communication/compute baseline or sanity check.\n",
        "- **Architecture flow:**\n",
        "  1. **Flatten** image from `(1, 28, 28)` to `784`\n",
        "  2. `Linear(784→50)` → `ReLU`\n",
        "  3. `Linear(50→10)` → **logits**\n",
        "- **Shape trace:** `(N, 1, 28, 28)` → `(N, 784)` → `(N, 50)` → `(N, 10)`\n",
        "- **Important fix:** In `__init__`, change `super(NetMnist, self).__init__()` to **`super(NetMnistFC, self).__init__()`** (or simply `super().__init__()`).\n",
        "\n",
        "---\n",
        "\n",
        "#### `BasicBlock` — ResNet-style residual block (for deeper backbones)\n",
        "- **Purpose:** A standard two-conv residual block with optional projection shortcut; typical of ResNet-18/34 (expansion=1).\n",
        "- **Architecture flow:**\n",
        "  1. `Conv2d(in→out, 3×3, stride, padding=1, no bias)` → `BatchNorm2d` → `ReLU`\n",
        "  2. `Conv2d(out→out, 3×3, stride=1, padding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33a8q9ACAWEn"
      },
      "outputs": [],
      "source": [
        "class NetMnist(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetMnist, self).__init__()\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)  # Conv1\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)  # Conv2\n",
        "        self.dropout2d = nn.Dropout2d()  # Dropout2d\n",
        "        # Layer 3\n",
        "        self.fc1 = nn.Linear(320, 50)  # FC1\n",
        "        self.dropout = nn.Dropout()  # Dropout\n",
        "        # Layer 4\n",
        "        self.fc2 = nn.Linear(50, 10)  # FC2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1 operations\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(x)\n",
        "        # Layer 2 operations\n",
        "        x = self.conv2(x)\n",
        "        x = self.dropout2d(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(x)\n",
        "        # Flatten\n",
        "        x = x.view(-1, 320)  # Flatten the tensor\n",
        "        # Layer 3 operations\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        # Layer 4 operation\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class NetMnistFC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetMnist, self).__init__()\n",
        "        # Input layer\n",
        "        self.fc1 = nn.Linear(784, 50)  # Fully connected layer from 784 to 50 neurons\n",
        "        # Hidden layer\n",
        "        self.fc2 = nn.Linear(50, 10)   # Fully connected layer from 50 to 10 neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the image from (1, 28, 28) to (784)\n",
        "        x = x.view(-1, 784)\n",
        "        # Forward pass through the first fully connected layer and apply ReLU activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # Forward pass through the second fully connected layer to produce output\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class NetCifar10(nn.Module):\n",
        "    \"\"\"\n",
        "    Compact CNN for CIFAR-10 tailored for Federated Learning:\n",
        "    - GroupNorm instead of BatchNorm (robust under non-IID, small local batches)\n",
        "    - 3 conv stages with MaxPool\n",
        "    - Dropout2d for mild regularization\n",
        "    - Global Average Pooling -> tiny classifier head\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, gn_groups=8, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        def conv_block(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "                nn.GroupNorm(num_groups=min(gn_groups, out_ch), num_channels=out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout2d(p=dropout_p)\n",
        "            )\n",
        "\n",
        "        # Stage 1: 32x32 -> 16x16\n",
        "        self.stem = nn.Sequential(\n",
        "            conv_block(3, 64),\n",
        "            conv_block(64, 64),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Stage 2: 16x16 -> 8x8\n",
        "        self.stage2 = nn.Sequential(\n",
        "            conv_block(64, 128),\n",
        "            conv_block(128, 128),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Stage 3: 8x8 -> 4x4\n",
        "        self.stage3 = nn.Sequential(\n",
        "            conv_block(128, 256),\n",
        "            conv_block(256, 256),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Global Average Pooling -> (B, 256, 1, 1)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc  = nn.Linear(256, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)    # -> (B, 64, 16, 16)\n",
        "        x = self.stage2(x)  # -> (B, 128, 8, 8)\n",
        "        x = self.stage3(x)  # -> (B, 256, 4, 4)\n",
        "        x = self.gap(x)     # -> (B, 256, 1, 1)\n",
        "        x = torch.flatten(x, 1)  # -> (B, 256)\n",
        "        x = self.fc(x)      # logits (B, num_classes)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading and Partitioning the Dataset\n",
        "\n",
        "This function `load_dataset` handles **three key tasks**:\n",
        "\n",
        "1. **Download & preprocess datasets**  \n",
        "   - Supports **MNIST**, **CIFAR-10**, and **Fashion-MNIST**.  \n",
        "   - Applies dataset-specific **normalization** to scale pixel values into ranges suitable for training.  \n",
        "   - Returns standard **PyTorch `Dataset`** objects and corresponding **`DataLoader`** wrappers:\n",
        "     - `train_loader` → batches of training samples (size = `BATCH_SIZE`) shuffled each epoch  \n",
        "     - `test_loader` → test set in large batches (size = 1000) for fast evaluation\n",
        "\n",
        "2. **Partition training data among clients**  \n",
        "   - Uses the **Dirichlet distribution** to split the training set indices into `n_particoes` subsets.  \n",
        "   - This simulates **non-IID data** across clients:  \n",
        "     - Each class’s samples are allocated unevenly to different clients.  \n",
        "     - The concentration parameter `dirichlet_alpha` controls the level of imbalance:  \n",
        "       - Small `α` → stronger heterogeneity (clients specialize in fewer classes).  \n",
        "       - Large `α` → more balanced splits across clients.  \n",
        "\n",
        "3. **Return datasets and partitions**  \n",
        "   - `train_dataset`, `test_dataset` → full datasets  \n",
        "   - `train_loader`, `test_loader` → DataLoaders for centralized training/evaluation  \n",
        "   - `idx` → list of index arrays, one per client, specifying which samples belong to which partition  \n",
        "\n",
        "---\n",
        "\n",
        "#### Why this matters for Federated Learning\n",
        "- Federated setups require each client to train on its **own subset of data**.  \n",
        "- By using Dirichlet-based splits, the experiment simulates realistic **data heterogeneity** across participants.  \n",
        "- This allows you to compare how different algorithms (e.g., **FedAvg** vs **FedP2PAvg**) cope with **non-IID and imbalanced** data distributions.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZISkV9nAWEn"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset, n_particoes, dirichlet_alpha):\n",
        "    train_dataset = None\n",
        "    test_dataset = None\n",
        "    train_loader = None\n",
        "    test_loader = None\n",
        "    idx = None\n",
        "    if dataset == 'mnist':\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=1000)\n",
        "    elif dataset == 'cifar10':\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                                        (0.2470, 0.2435, 0.2616))])\n",
        "        train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=1000)\n",
        "    elif dataset == 'fashionmnist':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=1000)\n",
        "\n",
        "    if train_dataset:\n",
        "        targets = np.array(train_dataset.targets)\n",
        "        classes = np.unique(targets)\n",
        "\n",
        "        idx = [[] for _ in range(n_particoes)]\n",
        "\n",
        "        for c in classes:\n",
        "            # class c indexes\n",
        "            indexes = np.where(targets == c)[0]\n",
        "            np.random.shuffle(indexes)\n",
        "\n",
        "            # Dirichlet proportions and initial integer counts\n",
        "            props = np.random.dirichlet([dirichlet_alpha] * n_particoes)\n",
        "            raw = props * len(indexes)\n",
        "            counts = raw.astype(int)\n",
        "\n",
        "            # redistributes the \"remainder\" to add up to exactly len(indexes)\n",
        "            deficit = len(indexes) - counts.sum()\n",
        "            if deficit > 0:\n",
        "                # gives the remainders to the largest fractional parts\n",
        "                fracs = raw - counts\n",
        "                order = np.argsort(-fracs)\n",
        "                for k in range(deficit):\n",
        "                    counts[order[k % n_particoes]] += 1\n",
        "\n",
        "            # slice and add\n",
        "            start = 0\n",
        "            for i, p in enumerate(counts):\n",
        "                if p > 0:\n",
        "                    idx[i].extend(indexes[start:start + p].tolist())\n",
        "                    start += p\n",
        "\n",
        "        # shuffles each partition\n",
        "        idx = [np.random.permutation(x).tolist() for x in idx]\n",
        "\n",
        "        # ensures no partition is empty (moves 1 sample from the largest \"donor\")\n",
        "        sizes = [len(x) for x in idx]\n",
        "        empties = [i for i, s in enumerate(sizes) if s == 0]\n",
        "        for e in empties:\n",
        "            donor = int(np.argmax(sizes))\n",
        "            if sizes[donor] > 1:\n",
        "                idx[e].append(idx[donor].pop())\n",
        "                sizes[donor] -= 1\n",
        "                sizes[e] += 1\n",
        "\n",
        "\n",
        "    return train_dataset, test_dataset, train_loader, test_loader, idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Factory + Client Initialization + Non-IID Split Visualization\n",
        "\n",
        "This cell provides three utilities you’ll reuse across experiments:\n",
        "\n",
        "1) **`loadNetwork(dataset, device)` — pick a backbone per dataset**  \n",
        "   - For `\"mnist\"` and `\"fashionmnist\"`, returns `NetMnist()`; for `\"cifar10\"`, returns `NetCifar10()`.  \n",
        "   - Moves the model to the specified `device` (e.g., `\"cuda\"` or `\"cpu\"`).  \n",
        "   - **Tip:** If you want to compare backbones, extend this to accept a `model_name` (e.g., `\"cnn\"`, `\"mlp\"`, `\"vgg11\"`) and route accordingly (you already have `NetMnistFC` available but not used here).\n",
        "\n",
        "2) **`configure_nets(dataset, n_clients, device)` — instantiate per-client models**  \n",
        "   - Builds a list of `n_clients` and a `net_central_node` **independent model instances** using `loadNetwork`.  \n",
        "   - In federated training, each client holds its own model copy; server aggregation will combine weights later.  \n",
        "   - **Note:** If you plan to do partial participation per round, you can still pre-create all clients once here.\n",
        "\n",
        "3) **`plot_class_distribution(idx, train_dataset, num_classes)` — sanity-check the Dirichlet split**  \n",
        "   - Expects `idx` to be a list of index lists (one per client/partition) from your Dirichlet partitioner.  \n",
        "   - Extracts labels from `train_dataset.targets` (works with both **`torch.Tensor`** and **Python lists**).  \n",
        "   - Computes per-class counts per partition, then renders a **stacked bar chart** so you can visually verify non-IID skew.  \n",
        "   - Legend shows **Class 0..(num_classes−1)**; x-axis lists partitions; y-axis shows sample counts.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why this matters\n",
        "- **Model allocation:** Each client must have a model instance to perform local updates before aggregation.  \n",
        "- **Correctness check for splits:** Visual confirmation helps catch mistakes like empty partitions, extreme imbalance beyond your chosen `DIRICHLET_ALPHA`, or mislabeled targets.\n",
        "\n",
        "---\n",
        "\n",
        "#### Practical tips & small fixes\n",
        "- If some partitions are **empty**, the stacked bars may be zero-height; you might want to assert minimum sizes or re-sample the Dirichlet proportions.\n",
        "- To compare backbones fairly across datasets, keep **identical optimizer and LR schedules** per setting, and only change the architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLuh7IY7AWEn"
      },
      "outputs": [],
      "source": [
        "def loadNetwork(dataset, device):\n",
        "    net = None\n",
        "    if dataset == 'mnist' or dataset == 'fashionmnist':\n",
        "        net = NetMnist().to(device)\n",
        "    elif dataset == 'cifar10':\n",
        "        net = NetCifar10().to(device)\n",
        "    return net\n",
        "\n",
        "def configure_nets(dataset, n_clients, device):\n",
        "    net_central_node = loadNetwork(dataset, device)\n",
        "    nets = []\n",
        "    for i in range(n_clients):\n",
        "        nets.append(loadNetwork(dataset, device))\n",
        "    return nets, net_central_node\n",
        "\n",
        "def plot_class_distribution(idx, train_dataset, num_classes):\n",
        "    \"\"\"\n",
        "    Plots a bar chart showing the class distribution across partitions.\n",
        "\n",
        "    Args:\n",
        "        idx (list of lists): Indices of the partitions.\n",
        "        train_dataset (Dataset): Dataset containing the labels.\n",
        "        num_classes (int): Number of classes (default: 10 for MNIST).\n",
        "    \"\"\"\n",
        "    # Calculate the class distribution for each partition\n",
        "    class_distributions = []\n",
        "    for partition_indices in idx:\n",
        "        part_targets = None\n",
        "        if isinstance(train_dataset.targets, torch.Tensor):\n",
        "            part_targets = [train_dataset.targets[index].item() for index in partition_indices]\n",
        "        else: \n",
        "            part_targets = [train_dataset.targets[index] for index in partition_indices]\n",
        "        class_counts = [part_targets.count(class_label) for class_label in range(num_classes)]\n",
        "        class_distributions.append(class_counts)\n",
        "\n",
        "    # Convert to numpy for easier handling\n",
        "    class_distributions = np.array(class_distributions)\n",
        "\n",
        "    # Configure the chart\n",
        "    num_partitions = len(idx)\n",
        "    partition_indices = range(num_partitions)\n",
        "    bar_width = 0.8\n",
        "\n",
        "    # Colors for each class\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, num_classes))\n",
        "\n",
        "    # Create stacked bars\n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "    bottom_values = np.zeros(num_partitions)\n",
        "    for class_idx in range(num_classes):\n",
        "        ax.bar(\n",
        "            partition_indices,\n",
        "            class_distributions[:, class_idx],\n",
        "            bottom=bottom_values,\n",
        "            color=colors[class_idx],\n",
        "            label=f'Class {class_idx}',\n",
        "            width=bar_width\n",
        "        )\n",
        "        bottom_values += class_distributions[:, class_idx]\n",
        "\n",
        "    # Chart configurations\n",
        "    ax.set_xlabel('Partition', fontsize=14)\n",
        "    ax.set_ylabel('Number of Samples', fontsize=14)\n",
        "    ax.set_title('Class Distribution Across Partitions', fontsize=16)\n",
        "    ax.set_xticks(partition_indices)\n",
        "    ax.set_xticklabels([f'Partition {i}' for i in partition_indices], rotation=45)\n",
        "    ax.legend(title='Classes', fontsize=10, loc='upper left', bbox_to_anchor=(1, 1))\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Local Training Loops: Independent vs. P2P Refinement\n",
        "\n",
        "This cell defines **two training routines** used by the hackathon baselines: a standard **per-client local training** loop and a minimal **peer-to-peer (P2P) refinement** loop. Both operate on **non-IID partitions** produced earlier and use **SGD + CrossEntropyLoss**.\n",
        "\n",
        "---\n",
        "\n",
        "#### `train_nets(nets, n_epochs, train_dataset, idx, device)`\n",
        "- **What it does:** Trains **each client’s own model** `nets[i]` on that client’s data partition `idx[i]` for `n_epochs`.  \n",
        "- **Data loader:** `DataLoader(Subset(train_dataset, idx[i]), batch_size=BATCH_SIZE, shuffle=True)` ensures each client only sees its assigned samples, shuffled every epoch.  \n",
        "- **Optimization:** For each client, creates a fresh `optimizer = optim.SGD(..., lr=LR, momentum=MOMENTUM)` and a `criterion = nn.CrossEntropyLoss()`.  \n",
        "- **Loop structure:**  \n",
        "  - Outer loop over clients.  \n",
        "  - Inner loop over `epoch` and mini-batches: forward → loss → backward → step.  \n",
        "- **Device:** Moves batches to `device` for GPU/CPU execution.  \n",
        "- **Returns:** The list `nets` with locally updated weights.\n",
        "\n",
        "---\n",
        "\n",
        "#### `train_p2p(nets, n_clients, n_epochs, train_dataset, idx, device)`\n",
        "- **What it does (current behavior):** For each partition `i`, **select a random peer `j`** and train **that peer’s model `nets[j]`** on partition `i`’s data for `n_epochs`. This mimics a simple P2P refinement step where models **learn from other clients’ distributions** before global aggregation.\n",
        "- **Peer selection:** `j = np.random.choice(range(n_clients))` → **uniform random** peer per partition.  \n",
        "- **Data loader:** Same construction as above but always using `idx[i]` (the data of the “source” client).  \n",
        "- **Optimization:** Fresh SGD and CE loss for the selected peer `nets[j]`.  \n",
        "- **Returns:** The list `nets` after all random peer refinements.\n",
        "\n",
        "**Important behavioral implications**\n",
        "- Because `j` is random, **some clients may be updated multiple times** in a pass, while others may not be updated at all.  \n",
        "- A client’s **own model `nets[i]` is not guaranteed** to be trained on `idx[i]` in this routine (only if `j == i`). This is intentional for P2P mixing, but keep it in mind when analyzing convergence.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Good practices & suggested improvements (optional)\n",
        "- **Optimizer state:** In P2P, re-creating the optimizer per mini-refinement **drops momentum history**. If you want momentum to carry over across multiple P2P updates of the **same** model within a round, keep a **dict of optimizers** keyed by client id.  \n",
        "- **Balance updates:** To ensure every peer is refined at least once, sample a **derangement/matching** or a **fixed number of updates per target peer** instead of i.i.d. random picks.  \n",
        "- **Criteria-based pairing:** Replace random `j` with a **selection policy** (e.g., topology neighbors, cosine similarity of weights/gradients, loss-based, or availability).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4sxZNHoAWEo"
      },
      "outputs": [],
      "source": [
        "def train_nets(nets, n_epochs, train_dataset, idx, device):\n",
        "    for i, net in enumerate(nets):\n",
        "        optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        part_loader = DataLoader(Subset(train_dataset, idx[i]), batch_size=BATCH_SIZE, shuffle=True)\n",
        "        for epoch in range(1, n_epochs):\n",
        "            net.train()\n",
        "            for batch_idx, (data, target) in enumerate(part_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = net(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "    return nets\n",
        "\n",
        "\n",
        "\n",
        "def train_p2p(nets, n_clients, n_epochs, train_dataset, idx, device):\n",
        "    for i in range(n_clients):\n",
        "        part_loader = DataLoader(Subset(train_dataset, idx[i]), batch_size=BATCH_SIZE, shuffle=True)\n",
        "        j = np.random.choice(range(n_clients))\n",
        "        net = nets[j]\n",
        "        optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        for epoch in range(n_epochs):\n",
        "            net.train()\n",
        "            for batch_idx, (data, target) in enumerate(part_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = net(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "    return nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Server-Side Aggregation (`combine`) and Centralized Evaluation (`eval`)\n",
        "\n",
        "This cell implements two standard server operations for federated learning:\n",
        "\n",
        "1) **`combine(nets, net_central_node)` — FedAvg-style aggregation + broadcast**  \n",
        "   - **Goal:** Average client model parameters to form a global model, copy the average into `net_central_node`, and then **broadcast** the global weights back to clients.  \n",
        "   - **Steps:**  \n",
        "     1. Create `average_weights` with the same shapes as the first client’s parameters.  \n",
        "     2. Accumulate parameters from all clients into `average_weights`.  \n",
        "     3. Divide by the number of clients to compute the mean.  \n",
        "     4. Copy the averaged weights into `net_central_node`.  \n",
        "     5. **Broadcast**: copy the global weights back to each client model so every client starts the next round from the same initialization.  \n",
        "\n",
        "2) **`eval(net_eval, test_loader, device)` — centralized test evaluation**  \n",
        "   - **Goal:** Compute average loss and accuracy of a given model on the test set.  \n",
        "   - **Details:**  \n",
        "     - Sets the model to `eval()` and disables gradients with `torch.no_grad()`.  \n",
        "     - Loops over the test loader, accumulates total loss, and counts correct predictions via `argmax`.  \n",
        "     - Prints and returns accuracy (in %).  \n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combine(nets, net_central_node):\n",
        "    with torch.no_grad():\n",
        "        average_weights = [torch.zeros_like(p) for p in net_central_node.parameters()]\n",
        "        for net in nets:\n",
        "            for avg_p, p in zip(average_weights, net.parameters()):\n",
        "                avg_p.add_(p.data)\n",
        "        for avg_p in average_weights:\n",
        "            avg_p.div_(len(nets))\n",
        "        for p, avg_p in zip(net_central_node.parameters(), average_weights):\n",
        "            p.data.copy_(avg_p)\n",
        "        for net in nets:\n",
        "            for p, p_node in zip(net.parameters(), net_central_node.parameters()):\n",
        "                p.data.copy_(p_node.data)\n",
        "            for buffer, buffer_node in zip(net.buffers(), net_central_node.buffers()):\n",
        "                buffer.copy_(buffer_node)\n",
        "    return nets, net_central_node\n",
        "\n",
        "def eval(net_eval, test_loader, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    net_eval.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = net_eval(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100.0 * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Preparation & Partition Visualization\n",
        "\n",
        "This cell loads the selected dataset, splits it into federated client partitions using a **Dirichlet distribution**, and then visualizes the resulting class distributions across clients. Specifically:\n",
        "\n",
        "- **`load_dataset(DATASET, N_CLIENTS, DIRICHLET_ALPHA)`**  \n",
        "  - Downloads and normalizes the chosen dataset (`\"mnist\"`, `\"fashionmnist\"`, `\"cifar10\"`).  \n",
        "  - Returns:\n",
        "    - `train_dataset`, `test_dataset` → full PyTorch dataset objects.  \n",
        "    - `train_loader`, `test_loader` → DataLoaders for centralized evaluation/debugging.  \n",
        "    - `idx` → list of index arrays, one per client, assigning samples to clients.  \n",
        "  - The `DIRICHLET_ALPHA` parameter controls **data heterogeneity**:  \n",
        "    - Smaller values (e.g., 0.05–0.1) → highly skewed partitions (non-IID).  \n",
        "    - Larger values (e.g., 0.5–10) → more balanced, closer to IID.\n",
        "\n",
        "- **`plot_class_distribution(idx, train_dataset, NUM_CLASSES)`**  \n",
        "  - Renders a stacked bar chart where each bar corresponds to one client partition.  \n",
        "  - Colors indicate per-class sample counts inside each client.  \n",
        "  - Useful to verify that the data split behaves as expected under the chosen Dirichlet α.\n",
        "\n",
        "**Why this step matters:** It ensures that each federated client trains only on its allocated data, while letting you *visualize the non-IID effect*—a key challenge for federated optimization algorithms like FedAvg and FedP2PAvg.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "vX7v4WRbAWEo",
        "outputId": "8041e0b6-be64-44c3-e3d7-d07d1735b1e7"
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset, train_loader, test_loader, idx = load_dataset(DATASET, N_CLIENTS, DIRICHLET_ALPHA)\n",
        "\n",
        "plot_class_distribution(idx, train_dataset, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Federated Training Loop: Local Updates → (Optional) P2P Refinement → FedAvg Combine → Central Eval\n",
        "\n",
        "This cell runs the **end-to-end federated training process** over `GLOBAL_ROUNDS`. Each round performs:\n",
        "1) **Local client training** on each client’s own partition (`train_nets`),  \n",
        "2) **(Optional) P2P refinement** where models learn from other clients’ distributions (`train_p2p` when `RUN_MODE == \"fedp2pavg\"`),  \n",
        "3) **Server-side aggregation + broadcast** (`combine`) to produce a new global model, and  \n",
        "4) **Centralized evaluation** of the global model on the test set (`eval`), logging accuracy over time.\n",
        "\n",
        "**Step-by-step**\n",
        "- `nets, central_node = configure_nets(DATASET, N_CLIENTS, DEVICE)`  \n",
        "  Initializes client models and the central/global model reference.  \n",
        "  > **Note:** Ensure `configure_nets` returns both `(nets, central_node)` or create `central_node = loadNetwork(DATASET, DEVICE)` separately if it currently returns only `nets`.\n",
        "\n",
        "- `for i in range(1, GLOBAL_ROUNDS):`  \n",
        "  Iterates training **rounds**. (Starts at 1; if you prefer the full count, use `range(GLOBAL_ROUNDS)`.)\n",
        "\n",
        "- `nets = train_nets(...)`  \n",
        "  Each client trains **locally** on its own data subset for `LOCAL_EPOCHS`.\n",
        "\n",
        "- `if RUN_MODE == \"fedp2pavg\": nets = train_p2p(...)`  \n",
        "  Optional **peer-to-peer** step: randomly selected peers are further trained on other partitions’ data to reduce drift and improve stability under **non-IID** splits.\n",
        "\n",
        "- `nets, central_node = combine(nets, central_node)`  \n",
        "  **FedAvg-style aggregation**: average client parameters into `central_node`, then **broadcast** the global weights back to all clients so they start the next round from the same state.\n",
        "\n",
        "- `accuracy = eval(central_node, test_loader, DEVICE)` → `accs.append(accuracy)`  \n",
        "  Centralized test-time evaluation of the global model; accuracy is recorded each round to `accs` for plotting/analysis.\n",
        "\n",
        "- Final `eval(central_node, test_loader, DEVICE)` and `print(accs)`  \n",
        "  Prints the last evaluation and the **accuracy trajectory** across rounds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6EWKt4fLna6",
        "outputId": "26d2987c-a19c-46ec-eb28-9b8c320a244d"
      },
      "outputs": [],
      "source": [
        "#separar o treino em configuração da rede e treino e treinar um pouco cada rede com seu próprio subconjunto antes de cada nova interação\n",
        "nets, central_node = configure_nets(DATASET, N_CLIENTS, DEVICE)\n",
        "\n",
        "accs = []\n",
        "for i in range(1, GLOBAL_ROUNDS):\n",
        "    print(f\"Epoch: {i}\")\n",
        "    nets = train_nets(nets, LOCAL_EPOCHS, train_dataset, idx, DEVICE)\n",
        "    if RUN_MODE == \"fedp2pavg\":\n",
        "        print(\"Starting p2p step\")\n",
        "        nets = train_p2p(nets, N_CLIENTS, LOCAL_EPOCHS, train_dataset, idx, DEVICE)\n",
        "    print(\"Combining\")\n",
        "    nets, central_node = combine(nets, central_node)\n",
        "    accuracy = eval(central_node, test_loader, DEVICE)\n",
        "    accs.append(accuracy)\n",
        "\n",
        "eval(central_node, test_loader, DEVICE)\n",
        "print(accs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
